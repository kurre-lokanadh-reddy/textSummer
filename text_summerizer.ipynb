{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import math\n",
    "#import networkx as nx   # FOR(PAGE RANK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readarticle(filename):\n",
    "    file=open(filename,'r')\n",
    "    filedata=file.read()\n",
    "    article=filedata.split(\".\")\n",
    "    sentences=[]\n",
    "    for sentence in article:\n",
    "        sentences.append(sentence.strip().replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "def readarticle_lema(filename):\n",
    "    file=open(filename,'r',encoding=\"utf-8\")\n",
    "    file_text=file.read()\n",
    "    file_text=re.sub(\"[()]\",\".\",file_text)\n",
    "    file_text=re.sub(\"[^a-zA-Z0-9.]\",\" \",file_text)\n",
    "    file_text=re.sub('[\"]',' ',file_text)\n",
    "    sents= file_text.split(\".\")\n",
    "    if len(sents[-1])<5:\n",
    "        sents.pop(-1)\n",
    "    words=list()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for sent in sents:\n",
    "        sent=sent.strip()\n",
    "        words.append([wordnet_lemmatizer.lemmatize(word.lower(), pos=\"v\") for word in sent.split() if word.lower() not in list(stopwords.words('english'))])\n",
    "    return words\n",
    "def readarticle_stma(filename):\n",
    "    file=open(filename,'r',encoding=\"utf-8\")\n",
    "    file_text=file.read()\n",
    "    file_text=re.sub(\"[()]\",\".\",file_text)\n",
    "    file_text=re.sub(\"[^a-zA-Z0-9.]\",\" \",file_text)\n",
    "    file_text=re.sub('[\"]',' ',file_text)\n",
    "    sents= file_text.split(\".\")\n",
    "    if len(sents[-1])<5:\n",
    "        sents.pop(-1)\n",
    "    words=list()\n",
    "    lancaster=LancasterStemmer()\n",
    "    for sent in sents:\n",
    "        sent=sent.strip()\n",
    "        words.append([lancaster.stem(word.lower()) for word in sent.split() if word.lower() not in list(stopwords.words('english'))])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniquewords(sentences):\n",
    "    stop_words=stopwords.words('english')\n",
    "    unque=set()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                unque.add(word)\n",
    "    return sorted(list(unque))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTIVE METHODS\n",
    "\n",
    "## 1 . TextRank (Page rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A,B):\n",
    "    return np.dot(A,B)/(np.linalg.norm(A)*np.linalg.norm(B))\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return cosine_similarity(vector1, vector2)\n",
    "    #return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        sent1=[i.lower() for i in sentences[idx1]]\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            sent2=[i.lower() for i in sentences[idx2]]\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sent1, sent2, stop_words)\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextRank_summary(file_name,ABSTRACT_SIZE=0.3):\n",
    "    stop_words=stopwords.words('english')\n",
    "    summary=[]\n",
    "    act_sentences,sentences=readarticle_stma(file_name)\n",
    "    similarityMatrix=build_similarity_matrix(sentences,stop_words)\n",
    "    \n",
    "    \n",
    "    #ranking the sentences using Page Rank\n",
    "    sentence_similarity_graph = nx.from_numpy_array(similarityMatrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    \n",
    "    #sorting according to the ranking\n",
    "    ranked_sentence = sorted(((scores[i],\" \".join(s)) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "    \n",
    "    return \".\".join([i[1] for i in ranked_sentence[:round(len(sentences)*ABSTRACT_SIZE)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'program also includ develop focus ai school provid bunch asset help build ai skil.envid three year collab program intellig cloud hub support around 100 institut ai infrastruct cours cont curricul develop support develop tool giv stud access cloud ai serv.attempt build ai ready workforc microsoft annount intellig cloud hub launch empow next gen stud ai ready skil.program develop provid job ready skil program want hon skil ai dat sci sery onlin cours feat hand lab expert instruct wel'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextRank_summary(\"text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 . Luhn's method (feature based )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(sentences):\n",
    "    record = {}\n",
    "    common_words =  stopwords.words('english')  #load_common_words()\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        for word in words:  #sentences is already a list of words so no need to split again. .\n",
    "            w = word.strip('.!?,()\\n').lower()\n",
    "            record[w]= record.get(w,0)+1\n",
    "\n",
    "    for word in record.keys():\n",
    "        if word in common_words:\n",
    "            record[word] = -1     \n",
    "    occur = [key for key in record.keys()]\n",
    "    occur.sort(reverse=True, key=lambda x: record[x])\n",
    "    return set(occur[: len(occur) // 10 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(sentence, metric):\n",
    "    words = sentence.split()\n",
    "    imp_words, total_words, begin_unimp, end, begin = [0]*5\n",
    "    for word in words:\n",
    "        w = word.strip('.!?,();').lower()\n",
    "        end += 1\n",
    "        if w in metric:\n",
    "            imp_words += 1\n",
    "            begin = total_words\n",
    "            end = 0\n",
    "        total_words += 1\n",
    "    unimportant = total_words - begin - end\n",
    "    if(unimportant != 0):\n",
    "        return float(imp_words**2) / float(unimportant)\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Luhn_summary(file_name,ABSTRACT_SIZE=0.3):\n",
    "    sentences = readarticle(file_name)\n",
    "    sentences = [\" \".join(sentence) for sentence in sentences] #to make words list to sentence\n",
    "    metric = top_words(sentences)\n",
    "    scores = {}\n",
    "    for sentence in sentences:\n",
    "        scores[sentence] = calculate_score(sentence, metric)\n",
    "    top_sentences =list(sentences) # make a copy\n",
    "    top_sentences.sort(key=lambda x: scores[x], reverse=True)      # sort by score\n",
    "    top_sentences = top_sentences[:round(len(scores)*ABSTRACT_SIZE)] # get top 5% (in persentage)\n",
    "    top_sentences.sort(key=lambda x: sentences.index(x))           # sort by occurrence\n",
    "    return '. '.join(top_sentences) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Luhn_summary(\"text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSA ( cross )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_tfidf(sentences , unique_words):\n",
    "    tf_idf= np.zeros((len(sentences),len(unique_words)))\n",
    "    tot_frequency=dict()\n",
    "    i=0\n",
    "    # frequency matrix or TF values\n",
    "    for sentence in sentences:\n",
    "        for word in sentence :\n",
    "            if word in unique_words:\n",
    "                j =unique_words.index(word)\n",
    "                freq = tf_idf[i][j]\n",
    "                if freq==0 :\n",
    "                    tot_frequency[word]=tot_frequency.get(word,0)+1\n",
    "                tf_idf[i][j]=freq+1\n",
    "        i=i+1\n",
    "    #print(tot_frequency)\n",
    "    #binary=tf_idf\n",
    "    # calculating IDF values for all the unique values\n",
    "    x,y = tf_idf.shape\n",
    "    idf={}\n",
    "    for i in tot_frequency.keys():\n",
    "        idf[i]=math.log(x/tot_frequency[i])\n",
    "    #print(idf)\n",
    "    # calculating tf_idf values \n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            tf_idf[i][j] = tf_idf[i][j]*idf[unique_words[j]]\n",
    "    \n",
    "    # modified Tf_IDF approch making the less than average values to zero to remove noise\n",
    "    sent_avg = np.mean(tf_idf,axis=1)\n",
    "    #print(\"average= \",sent_avg)\n",
    "    res=[]\n",
    "    for i in range(x):\n",
    "        res.append(list(np.greater(tf_idf[i],sent_avg[i]).astype(\"int\")))\n",
    "    #print(np.count_nonzero(tf_idf==0) , np.count_nonzero((res*tf_idf)==0))\n",
    "    return res*tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSA_summary(filename,ABSTRACT_SIZE=0.3):\n",
    "    sentences = readarticle(filename)\n",
    "    uniqueWords=get_uniquewords(sentences)\n",
    "    tf_idf_vectors=modified_tfidf(sentences,uniqueWords)\n",
    "    #print(len(sentences), len(uniqueWords), tf_idf_vectors.shape)\n",
    "    U,s,V = np.linalg.svd(np.transpose(tf_idf_vectors))\n",
    "    V_avg=np.mean(V,axis=1)\n",
    "    #print(\"avg= \" ,V_avg)\n",
    "    \n",
    "    # redusing the noices again \n",
    "    res=[]\n",
    "    for i in range(len(V_avg)):\n",
    "        res.append(list(np.greater(V[i],V_avg[i]).astype(\"int\")))\n",
    "    V= V*res\n",
    "    \n",
    "    # geting the sentence length values\n",
    "    Lengths = np.sum(V,axis=0)\n",
    "    #print(Lengths)\n",
    "    # Selecting the top sentences\n",
    "    sents_ord=sorted(sentences,key=lambda x: Lengths[sentences.index(x)] , reverse=True)\n",
    "    return (\".\".join([\" \".join(i) for i in sents_ord[:round(len(sentences)*ABSTRACT_SIZE) ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry.Earlier in April this year, the company announced Microsoft Professional Program In AI as a learning track open to the public.In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AI-ready skills.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSA_summary(\"text.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 . Fuzzi logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzyLogic.summerize import fuzzy_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before feature calculation\n",
      "feature 1 done\n",
      "feature 2 done\n",
      "feature 3 done\n",
      "feature 4 done\n",
      "feature 5 done\n",
      "feature 6 done\n",
      "feature 7 done\n",
      "feature 8 done\n",
      "after feature calculation ... going into results calculation\n",
      "after results vector is done almost done.... \n",
      "\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services.',\n",
       " 'The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.\"',\n",
       " 'The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_summary(\"text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lokanadh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lokanadh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Hybrid method OWN (under dev..) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_kvalue(vectors):\n",
    "    # using yellow brickes package\n",
    "    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_mean(pos,debug):\n",
    "    l=len(pos)\n",
    "    if(l<=1):\n",
    "        return pos[0]\n",
    "    else:\n",
    "        su=0\n",
    "        for i in range(1,len(pos)):\n",
    "            su=su+(pos[i]-pos[i-1])\n",
    "        return su/(l-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_removed_mean(pos,avg,debug):\n",
    "    if(debug):\n",
    "        print(pos)\n",
    "    l=len(pos)\n",
    "    if(l==1):\n",
    "        return pos[0]\n",
    "    left=pos[1]-pos[0]\n",
    "    right=pos[-1]-pos[-2]\n",
    "    if(left<=avg and right<=avg):\n",
    "        if(debug):\n",
    "            print(\"in equal\")\n",
    "        return sum(pos)/l\n",
    "    if(left>=right):\n",
    "        return outlier_removed_mean(pos[1:],avg,debug)\n",
    "    if(right>left):\n",
    "        return outlier_removed_mean(pos[:-1],avg,debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_flow(clusters,k,debug=False):\n",
    "    count={}\n",
    "    first_pos={}\n",
    "    last_pos={}\n",
    "    pos={}\n",
    "    for i in range(1,k):\n",
    "        count[i]=0\n",
    "        first_pos[i]=clusters.index(i)# calculating first position\n",
    "        last_pos[i]=0\n",
    "        pos[i]=[]\n",
    "    for i in range(len(clusters)):\n",
    "        count[clusters[i]]=count[clusters[i]]+1\n",
    "        last_pos[clusters[i]]=i\n",
    "        pos[clusters[i]].append(i)\n",
    "    if(debug):\n",
    "        print(\"counts=\",count)\n",
    "    #calculating spread (spread), positional average(pos_avg) , calculating the avgrage diff between the positions (pos_def_avg)\n",
    "    spread={}\n",
    "    pos_avg={}\n",
    "    pos_def_mean={}\n",
    "    for i in range(1,k):\n",
    "        spread[i]=last_pos[i]-first_pos[i]\n",
    "        pos_avg[i]=sum(pos[i])/len(pos[i])\n",
    "        pos_def_mean[i]= def_mean(pos[i],debug=debug)\n",
    "        \n",
    "    #calculating the new Pos_mean after removing the position outliers (w.r.t pos_def_avg)\n",
    "    final_pos={}\n",
    "    for i in range(1,k):\n",
    "        final_pos[i]=outlier_removed_mean(pos[i],pos_def_mean[i],debug=debug)\n",
    "    if(debug):\n",
    "        print(\"\\npositions = \",pos,\"\\nnormal avg= \",pos_avg,\"\\npos_deff=\",pos_def_mean,\"\\nfinal = \" ,final_pos)\n",
    "    flow=list(sorted(list(range(1,k)),key=lambda x: final_pos[x]))\n",
    "    if(debug):\n",
    "        print(flow)\n",
    "    \n",
    "    return flow,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 2 2 2 2 5 4 2 4 5 4 1 5 2 1 1 4 3 3 2 3 5 5 2 2 5 3 5 5 2 5 2 1 5 5 5 5\n",
      " 3 1 2]\n",
      "counts= {1: 5, 2: 12, 3: 5, 4: 4, 5: 14}\n",
      "[11, 14, 15, 32, 38]\n",
      "in equal\n",
      "[1, 2, 3, 4, 7, 13, 19, 23, 24, 29, 31, 39]\n",
      "[1, 2, 3, 4, 7, 13, 19, 23, 24, 29, 31]\n",
      "in equal\n",
      "[17, 18, 20, 26, 37]\n",
      "[17, 18, 20, 26]\n",
      "[17, 18, 20]\n",
      "in equal\n",
      "[6, 8, 10, 16]\n",
      "[6, 8, 10]\n",
      "in equal\n",
      "[0, 5, 9, 12, 21, 22, 25, 27, 28, 30, 33, 34, 35, 36]\n",
      "[5, 9, 12, 21, 22, 25, 27, 28, 30, 33, 34, 35, 36]\n",
      "[9, 12, 21, 22, 25, 27, 28, 30, 33, 34, 35, 36]\n",
      "[12, 21, 22, 25, 27, 28, 30, 33, 34, 35, 36]\n",
      "[21, 22, 25, 27, 28, 30, 33, 34, 35, 36]\n",
      "in equal\n",
      "\n",
      "positions =  {1: [11, 14, 15, 32, 38], 2: [1, 2, 3, 4, 7, 13, 19, 23, 24, 29, 31, 39], 3: [17, 18, 20, 26, 37], 4: [6, 8, 10, 16], 5: [0, 5, 9, 12, 21, 22, 25, 27, 28, 30, 33, 34, 35, 36]} \n",
      "normal avg=  {1: 22.0, 2: 16.25, 3: 23.6, 4: 10.0, 5: 22.642857142857142} \n",
      "pos_deff= {1: 6.75, 2: 3.4545454545454546, 3: 5.0, 4: 3.3333333333333335, 5: 2.769230769230769} \n",
      "final =  {1: 22.0, 2: 14.181818181818182, 3: 18.333333333333332, 4: 8.0, 5: 29.1}\n",
      "[4, 2, 3, 1, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([4, 2, 3, 1, 5], {1: 5, 2: 12, 3: 5, 4: 4, 5: 14})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "l=np.random.randint(low=1,high=6,size=40)\n",
    "print(l)\n",
    "\n",
    "find_flow(list(l),6,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_assertion(count,total_ratio,debug):#based on the cluster importances (pending)\n",
    "    if(debug):\n",
    "        print(sum(count.values()),\"*ratio  ->  \",sum(d.values())*0.3)\n",
    "\n",
    "    cl_ratios={}\n",
    "    for i,j in count.items():\n",
    "        cl_ratios[i]= round(j*total_ratio)\n",
    "    if(debug):\n",
    "        print(\"actual->\",sum(cl_ratios.values()))\n",
    "    return cl_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 *ratio  ->   51.6\n",
      "actual-> 52\n",
      "{1: 32, 2: 23, 3: 7, 4: 12, 5: 98}      {1: 10, 2: 7, 3: 2, 4: 4, 5: 29}\n"
     ]
    }
   ],
   "source": [
    "d={1:32,2:23,3:7,4:12,5:98}\n",
    "l=ratio_assertion(d,total_ratio=0.3,debug=True)\n",
    "print(d,\"    \",l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_own_summery(file_name,ABSTRACT_SIZE=0.3,debug=False):\n",
    "    sentences = readartical(file_name)\n",
    "    uniqueWords=get_uniquewords(sentences)\n",
    "    tf_idf_vectors=modified_tfidf(sentences,uniqueWords)\n",
    "    K = select_kvalue(tf_idf_vectors)\n",
    "    kmeans = KMeans(n_clusters=14).fit(text_tfidf)\n",
    "    clusters = kmeans.labels_\n",
    "    \n",
    "    path_order,cl_count = find_flow(clusters,K)\n",
    "    \n",
    "    cluster_ratios = ratio_assertion(cl_count,ABSTRACT_SIZE)\n",
    "    \n",
    "    summery=[]\n",
    "    for i in path_order:\n",
    "        summery.extend(top_sentences_clusterWise(cluster=i,cluster_ratio = cluster_ratios[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sentence Embidings (ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA SET AMAZON REVIEWS DATA SET (OPTIONAL)\n",
    "### PRE TRIENDED 50 DIMENSSIONS EMBIDINGS(GLOVE)\n",
    "### k-means clustering and selection based on the distance to the center of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def loadEmbeddingMatrix(EMBEDDING_FILE):\n",
    "    embeddings_index = dict()\n",
    "    #Transfer the embedding weights i dictionary by iterating through every line of the file.\n",
    "    f = open(EMBEDDING_FILE,'r',encoding='utf-8')\n",
    "    for line in f:\n",
    "        #split up line into an indexed array\n",
    "        values = line.split()\n",
    "        #first index is word\n",
    "        word = values[0]\n",
    "        #store the rest of the values in the array as a new array\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs #50 dimensions\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    return embeddings_index #, embedding_matrix\n",
    "\n",
    "## Loading 'glove' words\n",
    "emb_index= loadEmbeddingMatrix('C:/Users/lokanadh/Desktop/Anaconda/text summerization/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_embedding(wordlist):\n",
    "    \"\"\"\n",
    "    This function calculates the embedding of each sentence in the review. Checks if the sentence being passed is a valid one, \n",
    "    removing the punctuation and emojis etc.\n",
    "    \"\"\"\n",
    "    sent_emb = []\n",
    "    for i in wordlist:\n",
    "        i = i.lower()\n",
    "        try :\n",
    "            res=list(emb_index[i])\n",
    "        except:\n",
    "            res=list(emb_index['unknown'])\n",
    "        sent_emb.append(res)\n",
    "\n",
    "    #calculating the mean \n",
    "    sent_emb=np.mean(sent_emb,axis=0)\n",
    "    return np.array(sent_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AI-ready skills \" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry This program also included developer-focused AI school that provided a bunch of assets to help build AI skills '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "def Embeding_summery(file_name):\n",
    "    sentences =readarticle(file_name)\n",
    "    emb_sents=[get_sent_embedding(sent) for sent in sentences]\n",
    "    sentences=[\" \".join(sent) for sent in sentences]\n",
    "    n_clusters = int(np.ceil(len(emb_sents)**0.5))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(emb_sents)\n",
    "    avg = []\n",
    "    closest = []\n",
    "    for j in range(n_clusters):\n",
    "        idx = np.where(kmeans.labels_ == j)[0]\n",
    "        #print(\"IDX is: \", idx)\n",
    "        avg.append(np.mean(idx))\n",
    "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,emb_sents)\n",
    "    ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "    summary = ' '.join([sentences[closest[idx]] for idx in ordering])\n",
    "    return summary\n",
    "\n",
    "Embeding_summery(\"text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
